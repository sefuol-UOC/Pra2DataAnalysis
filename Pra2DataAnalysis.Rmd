---
title: 'PRA2 - Tipología y ciclo de vida de los datos: Limpieza y análisis de datos'
author: "Sergio Funes, David Ortiz"
date: "Junio de 2021"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
    highlight: default
    number_sections: no
    theme: cosmo
    includes:
      in_header: PRA_header.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)
```

```{r, include=FALSE}
library(VIM)
```

# 1. Descripción del dataset

El conjunto de datos es Dataset recruitment data [en Kaggle](https://www.kaggle.com/rafunlearnhub/recruitment-data). Está formado por 11 características que presentan 614 personas. Las características son las siguientes:

* **Número de serie**. Número del registro.

* **Género**. Si el sujeto es hombre o mujer.

* **Experiencia con Python**. Si el sujeto tiene experiencia de programación en Python.

* **Años de experiencia**. Número de años de experiencia del sujeto.

* **Educación**. Si el sujeto es graduado universitario o no.

* **Prácticas**. Si el sujeto ha hecho unas prácticas o no.

* **Puntuación**. Puntuación del sujeto.

* **Salario** (*10e4). Salario del sujeto, en decenas de miles de rupias.

* **Histórico de ofertas**. Si el sujeto ha tenido ofertas laborales.

* **Localización**. Tipo de localización del sujeto en relación con la ciudad: urbana, semiurbana o rural.

* **Estado de contratación**. Si el sujeto fue contratado o no.

A partir de este conjunto de datos, se pretende determinar que variables influyen más a la hora de decidir si un candidato es contratado o no. Además, se podrán crear modelos predictivos para predecir la decisión final.

Este análisis adquiere importancia a la hora de reclutar nuevos empleados en una empresa principalmente de informática, ya que a partir de su experiencia de programación, su educación y su experiencia, tendremos una primera aproximación sobre si un empleado será contratado o no.


# 2. Integración y selección de los datos de interés

Realizamos una lectura de los datos:

```{r}
data <- read.csv("recruitment_decision_tree.csv", header = TRUE, stringsAsFactors = TRUE)
head(data)
```

```{r}
sapply(data, function(x) class(x))
```

Observamos que los tipos de datos asignados son los correctos. A continuación, podemos prescindir de la variable Serial_no, ya que no nos aporta información sobre el candidato, únicamente representa que número de candidato es.

```{r}
data <- data[, -(1)]
```


# 3. Limpieza de los datos

## 3.1. Ceros y elementos vacíos

Comprobamos si nuestro conjunto de datos contiene elementos vacíos:

```{r}
sapply(data, function(x) sum(is.na(x)))
```

Podemos observar que hay 86 elementos vacíos entre las variables Experience_Years, Salary...10E4 y Offer_History. Procederemos a emplear un método de imputación de valores basada en k vecinos más próximos. Utilizaremos la función kNN de la libreria VIM:

```{r}
suppressWarnings(suppressMessages(library(VIM)))

data$Experience_Years <- kNN(data)$Experience_Years
data$Salary...10E4 <- kNN(data)$Salary...10E4
data$Offer_History <- kNN(data)$Offer_History

sapply(data, function(x) sum(is.na(x)))
```

Después de la imputación de valores, podemos comprobar que ya no existen valores vacíos en nuestro conjunto de datos.

## 3.2. Valores extremos

Identificaremos los valores outliers de dos formas diferentes, utilizando un diagrama de caja y utilizando la función boxplots.stats() de R. Representaremos los datos de las variables numéricas Score y Salary...10E4. Primeramente representaremos la variable Score:

```{r}
boxplot(data$Score)
boxplot.stats(data$Score)$out
```

Podemos observar que la variable Score contiene muchos outliers. En este caso, podemos deshacernos de los valores más lejanos, ya que aunque haya valores fuera del rango, no podemos asumir que sean outliers y no personas altamente cualificadas para ser contratadas. Para deshacernos de los outliers, le daremos valor vacío a partir de cierto rango y después utilizaremos la función kNN nuevamente para la imputación de los valores:

```{r}
data$Score[data$Score > 23000] <- NA

data$Score <- kNN(data)$Score

boxplot(data$Score)
```

Puede observarse que han desaparecido los outliers más lejanos. Continuaremos con la variable Salary...10E4:

```{r}
boxplot(data$Salary...10E4)
boxplot.stats(data$Salary...10E4)$out
```

```{r}
data$Salary...10E4[data$Salary...10E4 > 500] <- NA

data$Salary...10E4 <- kNN(data)$Salary...10E4

boxplot(data$Salary...10E4)
```

En la variable Salary...10E4, hemos realizado el mismo proceso que en la variable anterior.

## 3.3. Exportación de los datos preprocesados

Después de leer y validar el conjunto de datos, limpieza de los elementos vacíos y extremos, procedemos a guardar los datos en un fichero denominado "recruitment_decision_tree_clean.csv":

```{r}
write.csv(data, "recruitment_decision_tree_clean.csv")
```


# 4. Análisis de los datos

## 4.1. Selección de datos

Pasamos a la selección de las variables de interés para su análisis estadístico posterior. Encontramos aquí las variables categóricas más relevantes.

```{r}
# Per gender
data.women <- data[data$Gender == "Female", ]
data.men <- data[data$Gender == "Male", ]

# Per location type
data.urban <- data[data$Location == "Urban", ]
data.semiurban <- data[data$Location == "Semiurban", ]
data.rural <- data[data$Location == "Rural", ]

# Per education level
data.graduated <- data[data$Education == "Graduate", ]
data.not_graduated <- data[data$Education == "Not Graduate", ]

# Target var: recruitment status
data.recruited <- data[data$Recruitment_Status == "Y", ]
data.not_recruited <- data[data$Recruitment_Status == "N", ]
```

## 4.2. Comprobación de normalidad y homogeneidad de la varianza

Se comprueba ahora la normalidad de las variables cuantitativas, mediante el test de normalidad de Anderson-Darling. Si el p-valor de es superior al nivel de significancia de $\alpha = 0,05$, entonces podemos concluir que la variable en cuestión es normal.

```{r}
library(nortest)

num_vars = c("Experience_Years", "Score", "Salary...10E4")
alpha = 0.05

for (col in num_vars){
  p_value = ad.test(data[, col])$p.value
  if (p_value > alpha){
    print(paste0(col, ": Normal (p = ", p_value, ")"))
  } else {
    print(paste0(col, ": Not normal (p = ", p_value, ")"))
  }
}
```

Por tanto vemos que las tres variables numéricas siguen una distribución no normal.

Para estudiar la homogeneidad de las varianzas aplicamos el test de Fligner-Killeen. Se pretende comprobar esta en la relación del género de los candidatos con el salario asociado a estos. Para este test, la hipótesis nula, $H_0$, sostiene la igualdad de varianzas para ambos grupos. Por su parte, $H_1$ supondría varianzas diferentes.

Si el p-valor es superior a un nivel de significancia del 0,05, se aceptará dicha hipótesis nula y por tanto la igualdad de varianzas.

```{r}
fligner.test(Salary...10E4 ~ Gender, data=data)
```

Puesto que el p-valor es mayor a 0,05, se acepta la hipótesis nula y por tanto se asume la igualdad de varianzas para ambas muestras.


## 4.3. Pruebas estadísticas


# 5. Representación de resultados


# 6. Resolución y conclusiones


# 7. Código